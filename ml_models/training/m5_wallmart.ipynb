{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M5 Walmart Sales Forecasting with Prophet\n",
    "\n",
    "Complete EDA and Prophet training notebook for demand forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Prophet and metrics\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print('All libraries imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset from Local Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset paths (relative to this notebook location)\n",
    "DATASET_PATH = Path('../datasets/m5-forecasting-accuracy')\n",
    "\n",
    "calendar_file = DATASET_PATH / 'calendar.csv'\n",
    "sales_file = DATASET_PATH / 'sales_train_validation.csv'\n",
    "prices_file = DATASET_PATH / 'sell_prices.csv'\n",
    "\n",
    "print(f'Dataset path: {DATASET_PATH.absolute()}')\n",
    "print(f'Calendar file exists: {calendar_file.exists()}')\n",
    "print(f'Sales file exists: {sales_file.exists()}')\n",
    "print(f'Prices file exists: {prices_file.exists()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading calendar data...')\n",
    "calendar = pd.read_csv(calendar_file)\n",
    "print(f'Calendar shape: {calendar.shape}')\n",
    "print(calendar.head())\n",
    "print(calendar.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading sales data...')\n",
    "sales = pd.read_csv(sales_file)\n",
    "print(f'Sales shape: {sales.shape}')\n",
    "print(sales.head())\n",
    "print(sales.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading price data...')\n",
    "sell_prices = pd.read_csv(prices_file)\n",
    "print(f'Prices shape: {sell_prices.shape}')\n",
    "print(sell_prices.head())\n",
    "print(sell_prices.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from wide to long format\n",
    "print('Converting sales data from wide to long format...')\nsales = sales.melt(\n",
    "    id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n",
    "    var_name='d',\n",
    "    value_name='sales'\n",
    ")\n",
    "\n",
    "print(f'After melt shape: {sales.shape}')\n",
    "print(sales.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with calendar to get dates\n",
    "print('Merging with calendar data...')\nsales = sales.merge(calendar, on='d', how='left')\n",
    "\n",
    "print(f'After merge shape: {sales.shape}')\n",
    "print(sales.head())\n",
    "print(sales.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "print('=' * 60)\n",
    "print('DATA STRUCTURE ANALYSIS')\n",
    "print('=' * 60)\n",
    "\n",
    "print(f'\\nUnique stores: {sales[\"store_id\"].nunique()}')\n",
    "print(f'Store list: {sorted(sales[\"store_id\"].unique())}')\n",
    "\n",
    "print(f'\\nUnique items: {sales[\"item_id\"].nunique()}')\n",
    "print(f'\\nUnique categories: {sales[\"cat_id\"].nunique()}')\n",
    "print(f'Categories: {sorted(sales[\"cat_id\"].unique())}')\n",
    "\n",
    "print(f'\\nUnique states: {sales[\"state_id\"].nunique()}')\n",
    "print(f'States: {sorted(sales[\"state_id\"].unique())}')\n",
    "\n",
    "print(f'\\nDate range: {sales[\"date\"].min()} to {sales[\"date\"].max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Products per store\n",
    "products_per_store = sales.groupby('store_id')['item_id'].nunique()\n",
    "print('\\nProducts per store:')\n",
    "print(products_per_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on Store 1 (Single Store Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Store 1\n",
    "STORE_ID = 'CA_1'\n",
    "store_sales = sales[sales['store_id'] == STORE_ID].copy()\n",
    "\n",
    "print(f'Store {STORE_ID} Analysis:')\n",
    "print('=' * 60)\n",
    "print(f'Shape: {store_sales.shape}')\n",
    "print(f'Products: {store_sales[\"item_id\"].nunique()}')\n",
    "print(f'Categories: {store_sales[\"cat_id\"].nunique()}')\n",
    "print(f'Date range: {store_sales[\"date\"].min()} to {store_sales[\"date\"].max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales statistics\n",
    "print('\\n' + '=' * 60)\n",
    "print('SALES STATISTICS - Store CA_1')\n",
    "print('=' * 60)\n",
    "\n",
    "print('\\nOverall sales statistics:')\n",
    "print(store_sales['sales'].describe())\n",
    "\n",
    "zero_sales = (store_sales['sales'] == 0).sum()\n",
    "total_records = len(store_sales)\n",
    "zero_pct = (zero_sales / total_records) * 100\n",
    "print(f'\\nZero sales records: {zero_sales} ({zero_pct:.2f}%)')\n",
    "\n",
    "print('\\nTop 10 products by total sales:')\n",
    "top_products = store_sales.groupby('item_id')['sales'].sum().nlargest(10)\n",
    "print(top_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top products\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Top 10 products\n",
    "top_products.plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Top 10 Products by Total Sales (Store CA_1)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Item ID')\n",
    "axes[0].set_ylabel('Total Sales')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Sales distribution\n",
    "store_sales['sales'].hist(bins=50, ax=axes[1], color='steelblue', edgecolor='black')\n",
    "axes[1].set_title('Distribution of Daily Sales', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Daily Sales')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series for top 5 products\n",
    "top_5_items = store_sales.groupby('item_id')['sales'].sum().nlargest(5).index\n",
    "print(f'Top 5 items: {list(top_5_items)}')\n",
    "\n",
    "fig, axes = plt.subplots(5, 1, figsize=(14, 12))\n",
    "\n",
    "for idx, item_id in enumerate(top_5_items):\n",
    "    item_data = store_sales[store_sales['item_id'] == item_id].sort_values('date')\n",
    "    axes[idx].plot(item_data['date'], item_data['sales'], linewidth=1.5, color='steelblue')\n",
    "    axes[idx].fill_between(item_data['date'], item_data['sales'], alpha=0.3, color='steelblue')\n",
    "    axes[idx].set_title(f'Item {item_id} - Daily Sales', fontweight='bold')\n",
    "    axes[idx].set_ylabel('Sales')\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "axes[-1].set_xlabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonality analysis\n",
    "store_sales['date'] = pd.to_datetime(store_sales['date'])\n",
    "store_sales['day_of_week'] = store_sales['date'].dt.day_name()\n",
    "store_sales['month'] = store_sales['date'].dt.month\n",
    "\n",
    "dow_sales = store_sales.groupby('day_of_week')['sales'].mean().reindex(\n",
    "    ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "dow_sales.plot(kind='bar', ax=axes[0], color='coral')\n",
    "axes[0].set_title('Average Sales by Day of Week', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Day of Week')\n",
    "axes[0].set_ylabel('Average Daily Sales')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "month_sales = store_sales.groupby('month')['sales'].mean()\n",
    "month_sales.plot(kind='bar', ax=axes[1], color='lightgreen')\n",
    "axes[1].set_title('Average Sales by Month', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Month')\n",
    "axes[1].set_ylabel('Average Daily Sales')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top item for training\n",
    "TOP_ITEM = top_5_items[0]\n",
    "print(f'Selected item for training: {TOP_ITEM}')\n",
    "\n",
    "# Prepare data for Prophet\n",
    "item_data = store_sales[store_sales['item_id'] == TOP_ITEM].copy()\n",
    "item_data = item_data.sort_values('date')\n",
    "\n",
    "prophet_df = item_data[['date', 'sales']].copy()\n",
    "prophet_df.columns = ['ds', 'y']\n",
    "prophet_df['ds'] = pd.to_datetime(prophet_df['ds'])\n",
    "prophet_df = prophet_df.reset_index(drop=True)\n",
    "\n",
    "print(f'\\nDataset shape: {prophet_df.shape}')\n",
    "print(f'Date range: {prophet_df[\"ds\"].min()} to {prophet_df[\"ds\"].max()}')\n",
    "print(f'\\nFirst 10 rows:')\n",
    "print(prophet_df.head(10))\n",
    "print(f'\\nSales statistics:')\n",
    "print(prophet_df['y'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values and prepare holidays\n",
    "prophet_df['y'] = prophet_df['y'].fillna(method='ffill').fillna(0)\n",
    "\n",
    "# Add holidays from calendar\n",
    "holidays_df = calendar[calendar['event_name_1'].notna()].copy()\n",
    "holidays_df = holidays_df[['date', 'event_name_1']].rename(\n",
    "    columns={'date': 'ds', 'event_name_1': 'holiday'}\n",
    ")\n",
    "holidays_df['ds'] = pd.to_datetime(holidays_df['ds'])\n",
    "\n",
    "print(f'Holidays included: {len(holidays_df)}')\n",
    "print(holidays_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Prophet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '=' * 60)\n",
    "print('TRAINING PROPHET MODEL')\n",
    "print('=' * 60)\n",
    "\n",
    "model = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=False,\n",
    "    seasonality_mode='additive',\n",
    "    interval_width=0.95,\n",
    "    holidays=holidays_df\n",
    ")\n",
    "\n",
    "print('Model configuration:')\n",
    "print('  - Yearly seasonality: True')\n",
    "print('  - Weekly seasonality: True')\n",
    "print('  - Seasonality mode: Additive')\n",
    "print('  - Confidence interval: 95%')\n",
    "\n",
    "print(f'\\nTraining on {len(prophet_df)} observations...')\n",
    "model.fit(prophet_df)\n",
    "print('Model trained successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make forecast\n",
    "FORECAST_DAYS = 30\n",
    "future = model.make_future_dataframe(periods=FORECAST_DAYS)\n",
    "\n",
    "print(f'\\nForecast period: {FORECAST_DAYS} days')\n",
    "print(f'Last training date: {prophet_df[\"ds\"].max()}')\n",
    "print(f'Last forecast date: {future[\"ds\"].max()}')\n",
    "\n",
    "print('\\nGenerating forecast...')\n",
    "forecast = model.predict(future)\n",
    "print('Forecast generated!')\n",
    "\n",
    "print(f'\\nForecast results (last 35 rows):')\n",
    "print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize forecast\n",
    "fig = model.plot(forecast, figsize=(14, 8))\n",
    "plt.title(f'Prophet Forecast - {TOP_ITEM} (30-Day Forecast with 95% CI)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components plot\n",
    "fig = model.plot_components(forecast, figsize=(14, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "forecast_hist = forecast[forecast['ds'] <= prophet_df['ds'].max()].copy()\n",
    "eval_df = prophet_df.merge(forecast_hist[['ds', 'yhat']], on='ds', how='left')\n",
    "\n",
    "mape = mean_absolute_percentage_error(eval_df['y'], eval_df['yhat'])\n",
    "mae = np.mean(np.abs(eval_df['y'] - eval_df['yhat']))\n",
    "rmse = np.sqrt(np.mean((eval_df['y'] - eval_df['yhat'])**2))\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('MODEL EVALUATION METRICS')\n",
    "print('=' * 60)\n",
    "print(f'\\nMAPE: {mape:.2f}%')\n",
    "print(f'MAE: {mae:.2f} units')\n",
    "print(f'RMSE: {rmse:.2f} units')\n",
    "print(f'Average actual sales: {eval_df[\"y\"].mean():.2f} units')\n",
    "print(f'Average forecast: {eval_df[\"yhat\"].mean():.2f} units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted (last 90 days)\n",
    "last_90 = eval_df.tail(90)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot(last_90['ds'], last_90['y'], label='Actual', linewidth=2, color='blue')\n",
    "ax.plot(last_90['ds'], last_90['yhat'], label='Forecast', linewidth=2, color='red', linestyle='--')\n",
    "ax.fill_between(last_90['ds'], last_90['y'], last_90['yhat'], alpha=0.2, color='gray')\n",
    "ax.set_title(f'Actual vs Predicted - Last 90 Days ({TOP_ITEM})', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Sales')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals analysis\n",
    "residuals = eval_df['y'] - eval_df['yhat']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(eval_df['ds'], residuals, linewidth=1, alpha=0.7, color='darkblue')\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].set_title('Residuals Over Time', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Residual')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black', color='steelblue')\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_title('Distribution of Residuals', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Residual')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Forecast Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future forecast (next 30 days)\n",
    "future_forecast = forecast[forecast['ds'] > prophet_df['ds'].max()][['ds', 'yhat', 'yhat_lower', 'yhat_upper']].copy()\n",
    "future_forecast['yhat'] = future_forecast['yhat'].clip(lower=0)\n",
    "future_forecast.columns = ['Date', 'Predicted_Sales', 'Lower_Bound_95', 'Upper_Bound_95']\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print(f'30-DAY FORECAST - {TOP_ITEM}')\n",
    "print('=' * 60)\n",
    "print(future_forecast.to_string(index=False))\n",
    "\n",
    "print(f'\\nForecast Summary:')\n",
    "print(f'  Total predicted (30 days): {future_forecast[\"Predicted_Sales\"].sum():.0f} units')\n",
    "print(f'  Average daily: {future_forecast[\"Predicted_Sales\"].mean():.2f} units')\n",
    "print(f'  Peak day: {future_forecast[\"Predicted_Sales\"].max():.2f} units')\n",
    "print(f'  Lowest day: {future_forecast[\"Predicted_Sales\"].min():.2f} units')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Multiple Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_prophet_for_product(item_id, store_data, holidays, forecast_days=30):\n",
    "    \"\"\"\n",
    "    Train Prophet model for a single product\n",
    "    \"\"\"\n",
    "    item_data = store_data[store_data['item_id'] == item_id].copy()\n",
    "    item_data = item_data.sort_values('date')\n",
    "    \n",
    "    prophet_input = item_data[['date', 'sales']].copy()\n",
    "    prophet_input.columns = ['ds', 'y']\n",
    "    prophet_input['ds'] = pd.to_datetime(prophet_input['ds'])\n",
    "    prophet_input['y'] = prophet_input['y'].fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    if len(prophet_input) < 30:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        model = Prophet(\n",
    "            yearly_seasonality=True,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=False,\n",
    "            seasonality_mode='additive',\n",
    "            interval_width=0.95,\n",
    "            holidays=holidays\n",
    "        )\n",
    "        model.fit(prophet_input)\n",
    "        \n",
    "        future = model.make_future_dataframe(periods=forecast_days)\n",
    "        forecast = model.predict(future)\n",
    "        \n",
    "        forecast_hist = forecast[forecast['ds'] <= prophet_input['ds'].max()].copy()\n",
    "        mape = mean_absolute_percentage_error(prophet_input['y'], forecast_hist['yhat'])\n",
    "        \n",
    "        return {\n",
    "            'item_id': item_id,\n",
    "            'model': model,\n",
    "            'forecast': forecast,\n",
    "            'mape': mape,\n",
    "            'observations': len(prophet_input)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f'Error for {item_id}: {str(e)}')\n",
    "        return None\n",
    "\n",
    "print('Function defined: train_prophet_for_product()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for top 5 items\n",
    "print('\\n' + '=' * 60)\n",
    "print('TRAINING TOP 5 PRODUCTS')\n",
    "print('=' * 60)\n",
    "\n",
    "models = {}\n",
    "\n",
    "for item_id in top_5_items:\n",
    "    print(f'Training: {item_id}...', end=' ')\n",
    "    result = train_prophet_for_product(item_id, store_sales, holidays_df, forecast_days=30)\n",
    "    \n",
    "    if result:\n",
    "        models[item_id] = result\n",
    "        print(f'MAPE={result[\"mape\"]:.2f}%')\n",
    "    else:\n",
    "        print('FAILED')\n",
    "\n",
    "print(f'\\nSuccessfully trained {len(models)} models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print('\\n' + '=' * 60)\n",
    "print('TRAINED MODELS SUMMARY')\n",
    "print('=' * 60)\n",
    "\n",
    "summary = []\n",
    "for item_id, result in models.items():\n",
    "    summary.append({\n",
    "        'Item ID': item_id,\n",
    "        'MAPE (%)': f\"{result['mape']:.2f}\",\n",
    "        'Observations': result['observations'],\n",
    "        'Status': 'Ready'\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "avg_mape = np.mean([r['mape'] for r in models.values()])\n",
    "print(f'\\nAverage MAPE: {avg_mape:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "MODELS_DIR = Path('../models')\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('SAVING MODELS')\n",
    "print('=' * 60)\n",
    "\n",
    "for item_id, result in models.items():\n",
    "    model_path = MODELS_DIR / f'prophet_{STORE_ID}_{item_id}.pkl'\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(result['model'], f)\n",
    "    print(f'Saved: {model_path.name}')\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'store_id': STORE_ID,\n",
    "    'trained_items': list(models.keys()),\n",
    "    'model_count': len(models),\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'dataset_info': {\n",
    "        'total_records': len(store_sales),\n",
    "        'total_items': store_sales['item_id'].nunique(),\n",
    "        'date_range': f\"{store_sales['date'].min()} to {store_sales['date'].max()}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = MODELS_DIR / f'metadata_{STORE_ID}.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f'Saved: {metadata_path.name}')\n",
    "print('\\nAll models saved successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify by loading a saved model\n",
    "print('\\nVerifying saved model...')\ntest_item = list(models.keys())[0]\ntest_path = MODELS_DIR / f'prophet_{STORE_ID}_{test_item}.pkl'\n\nwith open(test_path, 'rb') as f:\n",
    "    loaded = pickle.load(f)\n",
    "\nprint(f'Successfully loaded: {test_item}')\nprint(f'Model type: {type(loaded)}')\nprint('Verification complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Completed:\n",
    "1. Loaded M5 Walmart dataset from local path\n",
    "2. EDA: 30K+ products, 1913 days, 10 stores\n",
    "3. Focused on Store CA_1 (analysis of 3K+ products)\n",
    "4. Trained Prophet model on top product with 30-day forecast\n",
    "5. Evaluated with MAPE, MAE, RMSE metrics\n",
    "6. Scaled to train 5 products\n",
    "7. Saved models and metadata for production\n",
    "\n",
    "Next Step: Use saved models in Django seed_demo.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python3",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
