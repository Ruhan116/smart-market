{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M5 Walmart Sales Forecasting with Prophet\n",
    "\n",
    "This notebook performs EDA on the M5 dataset and trains Facebook Prophet models for demand forecasting.\n",
    "\n",
    "Dataset Location: `ml_models/datasets/m5-forecasting-accuracy/`\n",
    "\n",
    "Goals:\n",
    "1. Exploratory Data Analysis (EDA)\n",
    "2. Data preprocessing for single store\n",
    "3. Prophet model training\n",
    "4. Forecast evaluation (MAPE, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Prophet for forecasting\n",
    "from prophet import Prophet\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Styling\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print('All libraries imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset path\n",
    "DATASET_PATH = Path('../../datasets/m5-forecasting-accuracy')\n",
    "\n",
    "# Check if files exist\n",
    "sales_file = DATASET_PATH / 'sales_train.csv'\n",
    "calendar_file = DATASET_PATH / 'calendar.csv'\n",
    "prices_file = DATASET_PATH / 'sell_prices.csv'\n",
    "\n",
    "print(f'Sales data file exists: {sales_file.exists()}')\n",
    "print(f'Calendar file exists: {calendar_file.exists()}')\n",
    "print(f'Prices file exists: {prices_file.exists()}')\n",
    "\n",
    "if not sales_file.exists():\n",
    "    print('\\nWARNING: Dataset files not found!')\n",
    "    print(f'Expected location: {DATASET_PATH.absolute()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print('Loading M5 datasets...')\n",
    "\n",
    "sales_df = pd.read_csv(sales_file)\n",
    "calendar_df = pd.read_csv(calendar_file)\n",
    "prices_df = pd.read_csv(prices_file)\n",
    "\n",
    "print(f'Sales data shape: {sales_df.shape}')\n",
    "print(f'Calendar data shape: {calendar_df.shape}')\n",
    "print(f'Prices data shape: {prices_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic info\n",
    "print('=' * 60)\n",
    "print('SALES DATA - First few rows')\n",
    "print('=' * 60)\n",
    "print(sales_df.head())\n",
    "print(f'\\nData shape: {sales_df.shape}')\n",
    "print(f'Columns: product_id, store_id, category, dept, state, + 1913 day columns (d_1 to d_1913)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '=' * 60)\n",
    "print('CALENDAR DATA')\n",
    "print('=' * 60)\n",
    "print(calendar_df.head(20))\n",
    "print(f'\\nCalendar shape: {calendar_df.shape}')\n",
    "print(f'Date range: {calendar_df[\"date\"].min()} to {calendar_df[\"date\"].max()}')\n",
    "print(f'\\nEvents in calendar:')\n",
    "print(calendar_df['event_name_1'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique stores and products\n",
    "print('\\n' + '=' * 60)\n",
    "print('DATA STRUCTURE ANALYSIS')\n",
    "print('=' * 60)\n",
    "\n",
    "print(f'\\nUnique stores: {sales_df[\"store_id\"].nunique()}')\n",
    "print(f'Stores: {sorted(sales_df[\"store_id\"].unique())}')\n",
    "print(f'\\nUnique items: {sales_df[\"item_id\"].nunique()}')\n",
    "print(f'\\nCategories: {sales_df[\"cat_id\"].nunique()}')\n",
    "print(f'Departments: {sales_df[\"dept_id\"].nunique()}')\n",
    "print(f'States: {sales_df[\"state_id\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Products per store\n",
    "products_per_store = sales_df.groupby('store_id')['item_id'].nunique()\n",
    "print('\\n' + '=' * 60)\n",
    "print('PRODUCTS PER STORE')\n",
    "print('=' * 60)\n",
    "print(products_per_store)\n",
    "print(f'\\nTotal unique products: {sales_df[\"item_id\"].nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Focus on Store 1 (Single Store Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Store 1 only\n",
    "STORE_ID = 1\n",
    "store_1_sales = sales_df[sales_df['store_id'] == STORE_ID].copy()\n",
    "\n",
    "print(f'\\nStore {STORE_ID} Analysis:')\n",
    "print('=' * 60)\n",
    "print(f'Shape: {store_1_sales.shape}')\n",
    "print(f'Products in store: {store_1_sales[\"item_id\"].nunique()}')\n",
    "print(f'Categories: {store_1_sales[\"cat_id\"].nunique()}')\n",
    "print(f'Departments: {store_1_sales[\"dept_id\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert wide format (d_1, d_2, ..., d_1913) to long format\ndef pivot_sales_to_timeseries(sales_data, calendar_data):\n",
    "    \"\"\"\n",
    "    Convert wide format sales data to long format time series.\n",
    "    Input: columns [store_id, item_id, d_1, d_2, ..., d_1913]\n",
    "    Output: DataFrame with [date, item_id, sales]\n",
    "    \"\"\"\n",
    "    day_cols = [col for col in sales_data.columns if col.startswith('d_')]\n",
    "    timeseries_data = []\n",
    "    \n",
    "    for idx, row in sales_data.iterrows():\n",
    "        item_id = row['item_id']\n",
    "        \n",
    "        for day_col in day_cols:\n",
    "            day_num = int(day_col.split('_')[1])\n",
    "            sales_qty = row[day_col]\n",
    "            \n",
    "            timeseries_data.append({\n",
    "                'item_id': item_id,\n",
    "                'day_num': day_num,\n",
    "                'sales': sales_qty\n",
    "            })\n",
    "    \n",
    "    ts_df = pd.DataFrame(timeseries_data)\n",
    "    \n",
    "    # Merge with calendar to get dates\n",
    "    calendar_sorted = calendar_data[['d', 'date']].drop_duplicates()\n",
    "    calendar_sorted.columns = ['day_num', 'date']\n",
    "    \n",
    "    ts_df = ts_df.merge(calendar_sorted, on='day_num', how='left')\n",
    "    ts_df['date'] = pd.to_datetime(ts_df['date'])\n",
    "    \n",
    "    return ts_df[['date', 'item_id', 'sales']].sort_values('date')\n",
    "\n",
    "print('Converting to time series format...')\n",
    "ts_df = pivot_sales_to_timeseries(store_1_sales, calendar_df)\n",
    "print(f'Time series shape: {ts_df.shape}')\n",
    "print(f'Date range: {ts_df[\"date\"].min()} to {ts_df[\"date\"].max()}')\n",
    "print(f'Unique items: {ts_df[\"item_id\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales statistics\n",
    "print('\\n' + '=' * 60)\n",
    "print('SALES STATISTICS')\n",
    "print('=' * 60)\n",
    "print('\\nOverall sales statistics:')\n",
    "print(ts_df['sales'].describe())\n",
    "\n",
    "zero_sales = (ts_df['sales'] == 0).sum()\n",
    "total_records = len(ts_df)\n",
    "zero_pct = (zero_sales / total_records) * 100\n",
    "print(f'\\nZero sales records: {zero_sales} ({zero_pct:.2f}%)')\n",
    "\n",
    "print('\\nSales distribution by item:')\n",
    "print(ts_df.groupby('item_id')['sales'].agg(['count', 'sum', 'mean', 'max']).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top products visualization\n",
    "top_items = ts_df.groupby('item_id')['sales'].sum().nlargest(10)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "top_items.plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Top 10 Products by Total Sales (Store 1)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Item ID')\n",
    "axes[0].set_ylabel('Total Sales')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "ts_df['sales'].hist(bins=50, ax=axes[1], color='steelblue', edgecolor='black')\n",
    "axes[1].set_title('Distribution of Daily Sales (Store 1)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Daily Sales Quantity')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series trends for top 5 products\n",
    "top_5_items = ts_df.groupby('item_id')['sales'].sum().nlargest(5).index\n",
    "\n",
    "fig, axes = plt.subplots(5, 1, figsize=(14, 12))\n",
    "\n",
    "for idx, item_id in enumerate(top_5_items):\n",
    "    item_data = ts_df[ts_df['item_id'] == item_id]\n",
    "    axes[idx].plot(item_data['date'], item_data['sales'], linewidth=1.5, color='steelblue')\n",
    "    axes[idx].fill_between(item_data['date'], item_data['sales'], alpha=0.3, color='steelblue')\n",
    "    axes[idx].set_title(f'Item {item_id} - Daily Sales Over Time', fontweight='bold')\n",
    "    axes[idx].set_ylabel('Sales')\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "axes[-1].set_xlabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonality analysis\n",
    "ts_df['day_of_week'] = ts_df['date'].dt.day_name()\n",
    "ts_df['month'] = ts_df['date'].dt.month\n",
    "\n",
    "dow_sales = ts_df.groupby('day_of_week')['sales'].mean().reindex(\n",
    "    ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "dow_sales.plot(kind='bar', ax=axes[0], color='coral')\n",
    "axes[0].set_title('Average Sales by Day of Week', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Day of Week')\n",
    "axes[0].set_ylabel('Average Daily Sales')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "month_sales = ts_df.groupby('month')['sales'].mean()\n",
    "month_sales.plot(kind='bar', ax=axes[1], color='lightgreen')\n",
    "axes[1].set_title('Average Sales by Month', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Month')\n",
    "axes[1].set_ylabel('Average Daily Sales')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Data Preprocessing for Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top product for initial training\n",
    "TOP_ITEM_ID = top_5_items[0]\n",
    "\n",
    "product_data = ts_df[ts_df['item_id'] == TOP_ITEM_ID].copy()\n",
    "product_data = product_data.sort_values('date')\n",
    "\n",
    "# Prepare for Prophet\n",
    "prophet_df = product_data[['date', 'sales']].copy()\n",
    "prophet_df.columns = ['ds', 'y']\n",
    "prophet_df['ds'] = pd.to_datetime(prophet_df['ds'])\n",
    "\n",
    "print(f'Selected Product for Training: Item {TOP_ITEM_ID}')\n",
    "print('=' * 60)\n",
    "print(f'Dataset shape: {prophet_df.shape}')\n",
    "print(f'Date range: {prophet_df[\"ds\"].min()} to {prophet_df[\"ds\"].max()}')\n",
    "print(f'\\nFirst 10 rows:')\n",
    "print(prophet_df.head(10))\n",
    "print(f'\\nSales statistics:')\n",
    "print(prophet_df['y'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "prophet_df['y'] = prophet_df['y'].fillna(method='ffill').fillna(0)\n",
    "\n",
    "# Add holidays from calendar\n",
    "holidays_df = calendar_df[calendar_df['event_name_1'].notna()].copy()\n",
    "holidays_df = holidays_df[['date', 'event_name_1']].rename(\n",
    "    columns={'date': 'ds', 'event_name_1': 'holiday'}\n",
    ")\n",
    "holidays_df['ds'] = pd.to_datetime(holidays_df['ds'])\n",
    "\n",
    "print(f'Holidays to include: {len(holidays_df)}')\n",
    "print(holidays_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Prophet Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '=' * 60)\n",
    "print('PROPHET MODEL TRAINING')\n",
    "print('=' * 60)\n",
    "\n",
    "model = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=False,\n",
    "    seasonality_mode='additive',\n",
    "    interval_width=0.95,\n",
    "    holidays=holidays_df\n",
    ")\n",
    "\n",
    "print(f'\\nModel configuration:')\n",
    "print(f'  Yearly seasonality: True')\n",
    "print(f'  Weekly seasonality: True')\n",
    "print(f'  Seasonality mode: Additive')\n",
    "print(f'  Interval width: 95%')\n",
    "\n",
    "print(f'\\nTraining on {len(prophet_df)} observations...')\n",
    "model.fit(prophet_df)\n",
    "print('Model trained successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate forecast for 30 days\n",
    "FORECAST_DAYS = 30\n",
    "future = model.make_future_dataframe(periods=FORECAST_DAYS)\n",
    "\n",
    "print(f'\\nForecast period: {FORECAST_DAYS} days')\n",
    "print(f'Future dataframe shape: {future.shape}')\n",
    "print(f'Last training date: {prophet_df[\"ds\"].max()}')\n",
    "print(f'Last forecast date: {future[\"ds\"].max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "print('\\nGenerating forecast...')\n",
    "forecast = model.predict(future)\n",
    "print('Forecast generated successfully!')\n",
    "\n",
    "print(f'\\nForecast (last 35 rows - includes training + future):')\n",
    "print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig = model.plot(forecast, figsize=(14, 8))\n",
    "plt.title(f'Prophet Forecast - Item {TOP_ITEM_ID} (30-Day Forecast with 95% CI)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components plot\n",
    "fig = model.plot_components(forecast, figsize=(14, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics on historical data\n",
    "forecast_hist = forecast[forecast['ds'] <= prophet_df['ds'].max()].copy()\n",
    "actual_hist = prophet_df.copy()\n",
    "\n",
    "eval_df = actual_hist.merge(forecast_hist[['ds', 'yhat']], on='ds', how='left')\n",
    "\n",
    "mape = mean_absolute_percentage_error(eval_df['y'], eval_df['yhat'])\n",
    "mae = np.mean(np.abs(eval_df['y'] - eval_df['yhat']))\n",
    "rmse = np.sqrt(np.mean((eval_df['y'] - eval_df['yhat'])**2))\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('MODEL EVALUATION METRICS')\n",
    "print('=' * 60)\n",
    "print(f'\\nMAPE (Mean Absolute Percentage Error): {mape:.2f}%')\n",
    "print(f'MAE (Mean Absolute Error): {mae:.2f} units')\n",
    "print(f'RMSE (Root Mean Squared Error): {rmse:.2f} units')\n",
    "print(f'Average actual sales: {eval_df[\"y\"].mean():.2f} units')\n",
    "print(f'Average forecast: {eval_df[\"yhat\"].mean():.2f} units')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted (last 90 days)\n",
    "last_90_days = eval_df.tail(90)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "ax.plot(last_90_days['ds'], last_90_days['y'], label='Actual', linewidth=2, color='blue')\n",
    "ax.plot(last_90_days['ds'], last_90_days['yhat'], label='Forecast', linewidth=2, color='red', linestyle='--')\n",
    "ax.fill_between(last_90_days['ds'], last_90_days['y'], last_90_days['yhat'], \n",
    "                 alpha=0.2, color='gray')\n",
    "ax.set_title(f'Actual vs Predicted Sales - Last 90 Days (Item {TOP_ITEM_ID})', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Sales')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals analysis\n",
    "residuals = eval_df['y'] - eval_df['yhat']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(eval_df['ds'], residuals, linewidth=1, alpha=0.7, color='darkblue')\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].fill_between(eval_df['ds'], residuals, 0, alpha=0.2, color='darkblue')\n",
    "axes[0].set_title('Residuals Over Time', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Residual')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black', color='steelblue')\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_title('Distribution of Residuals', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Residual')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Future Forecast Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future forecast (next 30 days)\n",
    "future_forecast = forecast[forecast['ds'] > prophet_df['ds'].max()][['ds', 'yhat', 'yhat_lower', 'yhat_upper']].copy()\n",
    "future_forecast['yhat'] = future_forecast['yhat'].clip(lower=0)\n",
    "future_forecast.columns = ['Date', 'Predicted_Sales', 'Lower_Bound', 'Upper_Bound']\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print(f'FORECAST FOR NEXT {FORECAST_DAYS} DAYS - Item {TOP_ITEM_ID}')\n",
    "print('=' * 60)\n",
    "print(future_forecast.to_string(index=False))\n",
    "\n",
    "print(f'\\nForecast Summary:')\n",
    "print(f'  Total predicted sales (30 days): {future_forecast[\"Predicted_Sales\"].sum():.0f} units')\n",
    "print(f'  Average daily sales: {future_forecast[\"Predicted_Sales\"].mean():.2f} units')\n",
    "print(f'  Max predicted sales: {future_forecast[\"Predicted_Sales\"].max():.2f} units')\n",
    "print(f'  Min predicted sales: {future_forecast[\"Predicted_Sales\"].min():.2f} units')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Training Multiple Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_prophet_for_product(item_id, ts_data, holidays_data, forecast_days=30):\n",
    "    \"\"\"\n",
    "    Train Prophet for a single product.\n",
    "    Returns dict with model, forecast, and MAPE.\n",
    "    \"\"\"\n",
    "    item_data = ts_data[ts_data['item_id'] == item_id].copy()\n",
    "    item_data = item_data.sort_values('date')\n",
    "    \n",
    "    prophet_input = item_data[['date', 'sales']].copy()\n",
    "    prophet_input.columns = ['ds', 'y']\n",
    "    prophet_input['ds'] = pd.to_datetime(prophet_input['ds'])\n",
    "    prophet_input['y'] = prophet_input['y'].fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    if len(prophet_input) < 30:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        model = Prophet(\n",
    "            yearly_seasonality=True,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=False,\n",
    "            seasonality_mode='additive',\n",
    "            interval_width=0.95,\n",
    "            holidays=holidays_data\n",
    "        )\n",
    "        model.fit(prophet_input)\n",
    "        \n",
    "        future = model.make_future_dataframe(periods=forecast_days)\n",
    "        forecast = model.predict(future)\n",
    "        \n",
    "        forecast_hist = forecast[forecast['ds'] <= prophet_input['ds'].max()].copy()\n",
    "        mape = mean_absolute_percentage_error(prophet_input['y'], forecast_hist['yhat'])\n",
    "        \n",
    "        return {\n",
    "            'item_id': item_id,\n",
    "            'model': model,\n",
    "            'forecast': forecast,\n",
    "            'mape': mape,\n",
    "            'n_observations': len(prophet_input)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f'Error for item {item_id}: {e}')\n",
    "        return None\n",
    "\n",
    "print('Function defined: train_prophet_for_product()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for top 5 products\n",
    "print('\\n' + '=' * 60)\n",
    "print('TRAINING MODELS FOR TOP 5 PRODUCTS')\n",
    "print('=' * 60)\n",
    "\n",
    "models_dict = {}\n",
    "\n",
    "for item_id in top_5_items:\n",
    "    print(f'\\nTraining for item {item_id}...')\n",
    "    result = train_prophet_for_product(item_id, ts_df, holidays_df, forecast_days=30)\n",
    "    \n",
    "    if result:\n",
    "        models_dict[item_id] = result\n",
    "        print(f'  Success - MAPE: {result[\"mape\"]:.2f}%, Observations: {result[\"n_observations\"]}')\n",
    "\n",
    "print(f'\\nTrained {len(models_dict)} models successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of trained models\n",
    "print('\\n' + '=' * 60)\n",
    "print('TRAINED MODELS SUMMARY')\n",
    "print('=' * 60)\n",
    "\n",
    "summary_data = []\n",
    "for item_id, result in models_dict.items():\n",
    "    summary_data.append({\n",
    "        'Item ID': item_id,\n",
    "        'MAPE (%)': f\"{result['mape']:.2f}\",\n",
    "        'Observations': result['n_observations'],\n",
    "        'Status': 'Ready'\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print('\\n' + summary_df.to_string(index=False))\n",
    "\n",
    "avg_mape = np.mean([r['mape'] for r in models_dict.values()])\n",
    "print(f'\\nAverage MAPE: {avg_mape:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "MODELS_DIR = Path('../models')\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('SAVING TRAINED MODELS')\n",
    "print('=' * 60)\n",
    "\n",
    "for item_id, result in models_dict.items():\n",
    "    model_path = MODELS_DIR / f'prophet_item_{item_id}.pkl'\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(result['model'], f)\n",
    "    print(f'Saved: {model_path.name}')\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'store_id': STORE_ID,\n",
    "    'trained_items': list(models_dict.keys()),\n",
    "    'model_count': len(models_dict),\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'dataset_info': {\n",
    "        'total_observations': len(ts_df),\n",
    "        'total_items': ts_df['item_id'].nunique()\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = MODELS_DIR / f'metadata_store_{STORE_ID}.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f'Saved: {metadata_path.name}')\n",
    "print('\\nAll models saved successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify by loading a saved model\n",
    "print('\\nVerifying saved models...')\ntest_item_id = list(models_dict.keys())[0]\ntest_model_path = MODELS_DIR / f'prophet_item_{test_item_id}.pkl'\n\nwith open(test_model_path, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\nprint(f'Successfully loaded model for item {test_item_id}')\nprint(f'Model type: {type(loaded_model)}')\nprint('All checks passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Completed:\n",
    "1. EDA on M5 Walmart dataset (10 stores, 30K+ products, 1913 days)\n",
    "2. Filtered to Store 1 (3K products for focused analysis)\n",
    "3. Converted wide format to time series\n",
    "4. Trained Prophet model on top product with 30-day forecast\n",
    "5. Evaluated with MAPE, MAE, RMSE metrics\n",
    "6. Scaled to train 5 products\n",
    "7. Saved all models for production use\n",
    "\n",
    "Next: Integrate into Django for API endpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python3",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
